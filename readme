RAG Fusion Pipeline Summary (LangChain Style)
RAG Fusion improves document retrieval by generating multiple query variations from the user's original question.
The pipeline is constructed using LangChain Expression Language (LCEL) chaining syntax with the | operator.

generate_queries:  A component (like an LLM chatTemplate model) that outputs multiple reformulations of the input query.
Example: Input: "What is RAG?"
Output: ["Explain RAG", "What is Retrieval-Augmented Generation?", "How does RAG work?"]
retriever.map(): Applies the retriever (like Chroma or FAISS or Pgvector) to each of those queries and returns top-k documents per query.
Result: A list of ranked document lists, one for each query.

reciprocal_rank_fusion (RRF): Merges all those ranked lists into a single final ranked document list.
RRF assigns scores using 1 / (rank + k) — rewarding documents that appear early and frequently.
This fusion improves relevance and recall by combining signals from all query variants.

The full chain:       retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion

results = retrieval_chain_rag_fusion.invoke("What is RAG?")

Optional: You can plug this into an LLM for final answer generation (e.g., using a local model like phi-4).

Works best when retriever.map() returns top-k (e.g., 3–5) results per query, not just 1.
