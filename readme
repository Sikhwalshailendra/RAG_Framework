
RAG -15 ( Rerank Fusion Method)
RAG Fusion Pipeline Summary (LangChain Style)
RAG Fusion improves document retrieval by generating multiple query variations from the user's original question.
The pipeline is constructed using LangChain Expression Language (LCEL) chaining syntax with the | operator.

generate_queries:  A component (like an LLM chatTemplate model) that outputs multiple reformulations of the input query.
Example: Input: "What is RAG?"
Output: ["Explain RAG", "What is Retrieval-Augmented Generation?", "How does RAG work?"]
retriever.map(): Applies the retriever (like Chroma or FAISS or Pgvector) to each of those queries and returns top-k documents per query.
Result: A list of ranked document lists, one for each query.

reciprocal_rank_fusion (RRF): Merges all those ranked lists into a single final ranked document list.
RRF assigns scores using 1 / (rank + k) — rewarding documents that appear early and frequently.
This fusion improves relevance and recall by combining signals from all query variants.

The full chain:       retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion

results = retrieval_chain_rag_fusion.invoke("What is RAG?")

Optional: You can plug this into an LLM for final answer generation (e.g., using a local model like phi-4).

Works best when retriever.map() returns top-k (e.g., 3–5) results per query, not just 1.







RAG-16 Corrective RAG:

Corrective-RAG (CRAG) is a strategy for RAG that incorporates self-reflection / self-grading on retrieved documents.
If at least one document exceeds the threshold for relevance during chatTemplate stage ( we use llm model and pass the specific template to check is the retrived document is useful or not and if all the retrived document 
from retrive is not useful then do some generateion from extra source like web or other), then it proceeds to generation
Before generation, it performs knowledge refinement
This partitions the document into "knowledge strips"
It grades each strip, and filters our irrelevant ones
If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource
It will use web search to supplement retrieval
We will implement some of these ideas from scratch using LangGraph:

Let's skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired.
If any documents are irrelevant, let's opt to supplement retrieval with web search.
We'll use Tavily Search for web search.
Let's use query re-writing to optimize the query for web search.



RAG - 17 ( self RAG):

 1. User asks a question
 2. Retrieve documents using a vector store
 3. Grade document relevance
 4. Generate an answer using a local LLM
 5. Detect hallucination and whether it answers the question
 6. If necessary, rewrite the question and repeat
  
